{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modul 1 - Mengimplementasikan NN dasar menggunakan library Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tujuan Modul:\n",
    "1. Mengenalkan cara kerja Jaringan Syaraf Tiruan\n",
    "2. Mengimplementasikan Jaringan Syaraf Tiruan untuk Klasifikasi Data Citra Sederhana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasifikasi angka/digit tulisan tangan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mengambil dan menyiapkan dataset MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset MNIST tersedia secara publik di http://yann.lecun.com/exdb/mnist/ dan terdiri dari empat bagian berikut:\n",
    "- Gambar set pelatihan: train-images-idx3-ubyte.gz (9,9 MB, 47 MB setelah diekstrak, 60.000 contoh)\n",
    "- Label set pelatihan: train-labels-idx1-ubyte.gz (29 KB, 60 KB setelah diekstrak, 60.000 label)\n",
    "- Gambar set pengujian: t10k-images-idx3-ubyte.gz (1,6 MB, 7,8 MB, 10.000 contoh)\n",
    "- Label set pengujian: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB setelah diekstrak, 10.000 label)\n",
    "\n",
    "Catatan: Ekstrak = unzip bagi yang tidak familiar dengan istilah ini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisasi ke rentang [-1, 1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memvisualisasikan digit pertama dari setiap kelas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as ____ # Silakan diisi bagian ini dengan kode yang tepat (5)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[y == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memvisualisasikan 25 versi berbeda dari digit \"7\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = ________________________  # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memisahkan dataset menjadi set pelatihan, validasi, dan pengujian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(________________) # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "\n",
    "\n",
    "# opsinal untuk mengosongkan memori dengan menghapus array yang tidak digunakan:\n",
    "del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mengimplementasikan multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementasi MLP secara Object Oriented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode berikut ini merupakan implementasi sederhana dari jaringan syaraf tiruan tipe **Multilayer Perceptron (MLP)** dengan satu lapisan tersembunyi dan fungsi aktivasi **sigmoid**. Pertama, terdapat fungsi `sigmoid(z)` yang digunakan untuk mengubah input numerik ke dalam rentang nilai antara 0 hingga 1, serta fungsi `int_to_onehot(y, num_labels)` yang mengonversi label kelas dalam bentuk bilangan bulat menjadi representasi **one-hot encoding** sehingga dapat digunakan dalam proses perhitungan loss.\n",
    "\n",
    "Kelas `NeuralNetMLP` menjadi inti dari implementasi ini. Pada tahap inisialisasi (`__init__`), jaringan dibangun dengan parameter berupa jumlah fitur (input), jumlah unit pada hidden layer, jumlah kelas output, serta seed acak untuk inisialisasi bobot. Bobot untuk lapisan tersembunyi (`weight_h`) dan lapisan keluaran (`weight_out`) diinisialisasi menggunakan distribusi normal dengan nilai kecil, sementara bias (`bias_h` dan `bias_out`) diinisialisasi dengan nol. Struktur bobot ini menentukan bagaimana input akan diproyeksikan ke hidden layer dan kemudian ke output layer.\n",
    "\n",
    "Metode `forward(x)` menjalankan proses **propagasi maju (forward pass)**. Input data dikalikan dengan bobot lapisan tersembunyi dan ditambahkan dengan bias untuk menghasilkan nilai linear (`z_h`), kemudian dilewatkan ke fungsi sigmoid sehingga menghasilkan aktivasi (`a_h`). Aktivasi hidden layer ini selanjutnya digunakan sebagai input ke lapisan keluaran untuk menghasilkan nilai prediksi (`a_out`).\n",
    "\n",
    "Selanjutnya, metode `backward(x, a_h, a_out, y)` berfungsi untuk menjalankan **propagasi balik (backpropagation)** guna menghitung gradien atau turunan parsial terhadap bobot dan bias. Proses ini dimulai dari lapisan keluaran dengan menghitung selisih antara output prediksi (`a_out`) dan label sebenarnya dalam bentuk one-hot (`y_onehot`). Dengan menggunakan turunan fungsi sigmoid, diperoleh error di output layer (`delta_out`). Gradien bobot dan bias lapisan keluaran kemudian dihitung dengan mengalikan error tersebut dengan aktivasi hidden layer. Error dari output layer juga dibawa kembali (backpropagated) ke hidden layer melalui bobot keluaran untuk menghitung error di hidden layer (`delta_h`). Dengan cara serupa, gradien bobot dan bias hidden layer diperoleh dari perkalian antara error hidden dengan input asli. Hasil akhirnya berupa empat gradien utama: gradien bobot keluaran, gradien bias keluaran, gradien bobot hidden, dan gradien bias hidden, yang akan digunakan untuk memperbarui bobot dalam proses training.\n",
    "\n",
    "Secara keseluruhan, kode ini menunjukkan alur dasar jaringan syaraf tiruan: mulai dari inisialisasi bobot, melakukan forward pass untuk menghitung output, kemudian backward pass untuk menghitung error dan gradien. Walaupun sederhana, implementasi ini sudah mencakup inti dari cara kerja MLP dalam pembelajaran mesin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for i, val in enumerate(y):\n",
    "        ary[i, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "\n",
    "class NeuralNetMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, random_seed=123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # lapisan tersembunyi (Hidden layer)\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        self.weight_h = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_hidden, num_features)) # Ini akan menghasilkan matriks bobot (weight) dengan dimensi [n_hidden, n_features]\n",
    "        self.bias_h = np.zeros(num_hidden)\n",
    "        \n",
    "        # lapisan keluaran (Output layer)\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Lapisan tersembunyi (Hidden layer)\n",
    "        # dimensi input: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # dimensi output: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        a_h = sigmoid(z_h)\n",
    "\n",
    "        # Lapisan keluaran (Output layer)\n",
    "        # dimensi input: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # dimensi output: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_h, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_out, y):\n",
    "    \n",
    "        #########################\n",
    "        ### Bobot lapisan keluaran (Output layer weights)\n",
    "        #########################\n",
    "        \n",
    "        # representasi one-hot (one-hot encoding)\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        # Bagian 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## di mana DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## untuk mempermudah penggunaan kembali (re-use)\n",
    "        \n",
    "        # dimensi input/output: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = 2.*(a_out - y_onehot) / y.shape[0]\n",
    "\n",
    "        # dimensi input/output: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = a_out * (1. - a_out) # turunan fungsi sigmoid (sigmoid derivative)\n",
    "\n",
    "        # dimensi output: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"placeholder aturan delta (delta rule)\"\n",
    "\n",
    "        # gradien untuk bobot keluaran (output weights)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "        \n",
    "        # dimensi input: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # dimensi output: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        \n",
    "\n",
    "        #################################\n",
    "        # Bagian 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        \n",
    "        # dimensi output: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = a_h * (1. - a_h) # turunan fungsi sigmoid (sigmoid derivative)\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # dimensi output: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out, \n",
    "                d_loss__d_w_h, d_loss__d_b_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetMLP(___________) # Silakan diisi bagian ini dengan kode yang tepat (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat kode loop pelatihan jaringan saraf tiruan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defenisikan data loaders:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di bawah ini merupakan proses **training model** menggunakan pendekatan **minibatch gradient descent**. Pertama, ditentukan dua parameter utama: `num_epochs = 50`, yang menyatakan bahwa pelatihan akan berlangsung selama 50 siklus penuh (epoch), dan `minibatch_size = 100`, yang artinya data latih akan dibagi menjadi kelompok kecil berisi 100 sampel pada setiap iterasi.\n",
    "\n",
    "Fungsi `minibatch_generator(X, y, minibatch_size)` bertugas untuk membagi dataset menjadi minibatch. Caranya, fungsi ini terlebih dahulu membuat array indeks dari semua data, kemudian mengacak urutan indeks dengan `np.random.shuffle(indices)` agar minibatch yang terbentuk bersifat acak dan tidak selalu sama urutannya. Setelah itu, fungsi menggunakan perulangan `for start_idx in range(...)` untuk mengambil potongan indeks sesuai ukuran minibatch (`minibatch_size`). Dengan cara ini, setiap kali fungsi dipanggil, ia akan menghasilkan (`yield`) sepasang potongan data fitur (`X[batch_idx]`) dan label (`y[batch_idx]`) yang siap digunakan dalam proses training.\n",
    "\n",
    "Selanjutnya, kode utama menjalankan perulangan **epoch** melalui `for i in range(num_epochs):`, yang berarti model akan melakukan training penuh sebanyak jumlah epoch yang ditentukan. Di dalamnya, dibuat generator minibatch dengan memanggil `minibatch_generator(X_train, y_train, minibatch_size)`. Generator ini kemudian diiterasi dengan perulangan `for X_train_mini, y_train_mini in minibatch_gen:`, sehingga pada setiap iterasi diperoleh satu minibatch data latih dan label yang bisa dipakai untuk update parameter model.\n",
    "\n",
    "Namun, dalam kode ini terdapat perintah `break` pada loop minibatch, sehingga perulangan hanya akan mengambil **satu minibatch pertama** dari data latih pada epoch tersebut. Kemudian ada `break` kedua pada loop epoch, yang menyebabkan proses training hanya berhenti pada **epoch pertama saja**. Dengan kata lain, meskipun ditentukan `num_epochs = 50`, kode ini hanya akan mengambil **1 minibatch dari epoch pertama** lalu langsung keluar. Hal ini kemungkinan besar hanya untuk **tujuan pengujian (debugging)**, agar programmer bisa memastikan bahwa fungsi `minibatch_generator` berjalan dengan benar sebelum dipakai untuk training penuh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# melakukan iterasi (perulangan) pada training epochs (jumlah siklus pelatihan)\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # melakukan iterasi pada minibatches (potongan kecil data latih)\n",
    "    minibatch_gen = minibatch_generator(____________) # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mendefinisikan fungsi untuk menghitung nilai kerugian (loss) dan akurasi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode berikut digunakan untuk **evaluasi performa model** setelah proses training selesai, dengan menghitung **loss** dan **akurasi** pada data validasi.\n",
    "\n",
    "Pertama, didefinisikan fungsi `mse_loss(targets, probas, num_labels=10)`. Fungsi ini menghitung **Mean Squared Error (MSE)** antara label sebenarnya dan output probabilitas dari model. Karena label target biasanya berupa bilangan bulat (misalnya 0, 1, 2 untuk klasifikasi digit), fungsi `int_to_onehot` dipanggil untuk mengubahnya menjadi representasi **one-hot encoding**. Selanjutnya, selisih antara vektor one-hot target dan probabilitas prediksi (`probas`) dihitung, lalu dipangkatkan dua dan dirata-ratakan menggunakan `np.mean`. Nilai MSE ini menunjukkan seberapa jauh distribusi probabilitas prediksi model dari label yang benar, di mana nilai yang lebih kecil berarti prediksi model semakin akurat.\n",
    "\n",
    "Kedua, terdapat fungsi `accuracy(targets, predicted_labels)` yang digunakan untuk menghitung tingkat akurasi prediksi. Akurasi dihitung dengan membandingkan label prediksi (`predicted_labels`) dengan label sebenarnya (`targets`). Jika prediksi sama dengan target, hasilnya `True` (yang dihitung sebagai 1), jika berbeda maka `False` (dihitung sebagai 0). Nilai rata-ratanya (`np.mean`) memberikan proporsi prediksi yang benar, sehingga diperoleh nilai akurasi antara 0 dan 1.\n",
    "\n",
    "Bagian eksekusi kode berikutnya menjalankan evaluasi pada data validasi. Pertama, `model.forward(X_valid)` dipanggil untuk melakukan **forward pass** pada data validasi (`X_valid`), dan hasilnya adalah aktivasi output berupa probabilitas (`probas`). Fungsi `mse_loss` kemudian digunakan untuk menghitung nilai MSE dengan `y_valid` sebagai label target dan `probas` sebagai hasil prediksi probabilitas. Selanjutnya, `np.argmax(probas, axis=1)` digunakan untuk mengambil label kelas dengan probabilitas tertinggi sebagai prediksi akhir (`predicted_labels`). Prediksi ini dibandingkan dengan label sebenarnya (`y_valid`) menggunakan fungsi `accuracy` untuk menghitung nilai akurasi (`acc`).\n",
    "\n",
    "Dengan demikian, kode ini memberikan dua metrik evaluasi:\n",
    "\n",
    "* **MSE (Mean Squared Error):** mengukur selisih kuadrat antara distribusi probabilitas prediksi dan label target.\n",
    "* **Accuracy:** mengukur proporsi prediksi yang tepat terhadap seluruh data validasi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets)\n",
    "\n",
    "\n",
    "_, probas = model.forward(X_valid)\n",
    "mse = mse_loss(__________________) # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(_________________) # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menggabungkan fungsi untuk menghitung nilai kerugian (loss) dan akurasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kode di bawah ini berfungsi untuk menghitung **nilai rata-rata MSE (Mean Squared Error)** dan **akurasi** dari model (`nnet`) pada suatu dataset (`X`, `y`) dengan cara memproses data dalam bentuk **minibatch**. Hal ini berguna ketika dataset berukuran besar sehingga tidak efisien untuk diproses sekaligus.\n",
    "\n",
    "Fungsi `compute_mse_and_acc` menerima beberapa parameter:\n",
    "\n",
    "* `nnet`: objek neural network (misalnya `NeuralNetMLP`) yang sudah dilatih.\n",
    "* `X` dan `y`: data fitur dan label target yang akan dievaluasi.\n",
    "* `num_labels`: jumlah kelas (default = 10, misalnya untuk klasifikasi digit MNIST).\n",
    "* `minibatch_size`: ukuran minibatch untuk memproses data secara bertahap.\n",
    "\n",
    "Di awal fungsi, tiga variabel inisialisasi dibuat:\n",
    "\n",
    "* `mse = 0.` untuk menyimpan total nilai loss MSE dari semua minibatch.\n",
    "* `correct_pred = 0` untuk menghitung jumlah prediksi yang benar.\n",
    "* `num_examples = 0` untuk menghitung jumlah total sampel yang diproses.\n",
    "\n",
    "Selanjutnya, generator minibatch dipanggil melalui `minibatch_generator(X, y, minibatch_size)`. Lalu dilakukan perulangan pada setiap minibatch (`for i, (features, targets) in enumerate(minibatch_gen):`).\n",
    "\n",
    "Di dalam loop:\n",
    "\n",
    "1. Model melakukan forward pass dengan `nnet.forward(features)` untuk mendapatkan probabilitas output (`probas`).\n",
    "2. Prediksi label kelas diperoleh dari `np.argmax(probas, axis=1)`, yaitu mengambil kelas dengan probabilitas tertinggi.\n",
    "3. Label target asli diubah ke one-hot menggunakan `int_to_onehot`.\n",
    "4. Loss MSE dihitung sebagai rata-rata kuadrat selisih antara one-hot target dan probabilitas prediksi (`np.mean((onehot_targets - probas)**2)`).\n",
    "5. Jumlah prediksi yang benar dihitung dengan `(predicted_labels == targets).sum()` dan ditambahkan ke `correct_pred`.\n",
    "6. Jumlah sampel pada minibatch (`targets.shape[0]`) ditambahkan ke `num_examples`.\n",
    "7. Nilai loss MSE dari minibatch ditambahkan ke total `mse`.\n",
    "\n",
    "Setelah semua minibatch selesai diproses, nilai rata-rata MSE dihitung dengan `mse/(i+1)`, yaitu total loss dibagi jumlah minibatch. Akurasi (`acc`) dihitung dengan `correct_pred/num_examples`, yaitu jumlah prediksi benar dibagi jumlah total sampel. Akhirnya, fungsi mengembalikan pasangan `(mse, acc)` sebagai hasil evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/(i+1)\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, acc = compute_mse_and_acc(___________________) # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dan Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi `train` menjalankan proses pelatihan model MLP dengan **minibatch gradient descent** selama `num_epochs`. Di awal, tiga list disiapkan untuk menyimpan riwayat metrik per-epoch: `epoch_loss` (MSE latih), `epoch_train_acc` (akurasi latih), dan `epoch_valid_acc` (akurasi validasi). Untuk setiap epoch, data latih dipecah menjadi minibatch melalui `minibatch_generator(X_train, y_train, minibatch_size)`. Pada setiap minibatch, model melakukan **forward pass** (`a_h, a_out = model.forward(...)`) guna memperoleh aktivasi hidden dan probabilitas keluaran, kemudian **backward pass** (`model.backward(...)`) untuk menghitung gradien terhadap bobot dan bias di kedua lapisan. Parameter diperbarui dengan **update aturan gradien** sederhana: `param -= learning_rate * grad`, mencakup `weight_h`, `bias_h`, `weight_out`, dan `bias_out`.\n",
    "\n",
    "Sesudah seluruh minibatch pada satu epoch selesai, fungsi melakukan **evaluasi penuh** pada set latih dan validasi menggunakan `compute_mse_and_acc`. Dari sini didapat `train_mse`, `train_acc`, `valid_mse`, dan `valid_acc` (dua yang terakhir hanya akurasi yang disimpan, sementara MSE validasi dihitung namun tidak dipakai untuk logging). Nilai akurasi dikonversi ke persen, lalu semua metrik kunci dicatat ke dalam list riwayat dan dicetak dalam format ringkas: nomor epoch, MSE latih, akurasi latih, dan akurasi validasi. Di akhir pelatihan, fungsi mengembalikan tiga deret metrik historis: `epoch_loss`, `epoch_train_acc`, dan `epoch_valid_acc` yang bermanfaat untuk **plot kurva learning** serta memantau **overfitting** (misalnya jika akurasi latih naik namun validasi stagnan/menurun).\n",
    "\n",
    "Catatan kecil: fungsi memakai variabel `minibatch_size` yang **tidak didefinisikan sebagai argumen** `train` (mengandalkan variabel global). Praktik yang lebih rapi adalah menjadikannya parameter fungsi. Selain itu, untuk tugas klasifikasi, MSE dapat bekerja tetapi umumnya **cross-entropy** dengan softmax lebih stabil; Anda juga bisa menambahkan fitur lanjutan seperti **early stopping**, **learning-rate schedule**, dan **shuffle per epoch** (yang sudah terjadi di generator) untuk hasil pelatihan yang lebih baik.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # melakukan iterasi pada minibatches (potongan kecil data latih)\n",
    "        minibatch_gen = minibatch_generator(\n",
    "            X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "            \n",
    "            #### Hitung keluaran (Compute outputs) ####\n",
    "            a_h, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Hitung gradien (Compute gradients) ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_out, y_train_mini)\n",
    "\n",
    "            #### Perbarui bobot (Update weights) ####\n",
    "            model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "        \n",
    "        #### Pencatatan per-epoch (Epoch Logging) ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) # untuk pengacakan (shuffling) data latih\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(____________________________,\n",
    "    num_epochs=50, learning_rate=0.1) # Silakan diisi bagian ini dengan kode yang tepat (15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi performa jaringan saraf tiruan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n",
    "         label='Training')\n",
    "plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n",
    "         label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penjelasan Grafik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafik di atas menunjukkan perkembangan akurasi model pada data latih (Training) dan data validasi (Validation) selama 50 epoch. Terlihat bahwa pada awal epoch akurasi meningkat tajam dari sekitar 76% hingga lebih dari 85%, kemudian kenaikan menjadi lebih landai seiring bertambahnya epoch. Baik kurva training maupun validation menunjukkan tren yang konsisten meningkat hingga mencapai kisaran 95%. Perbedaan akurasi antara training dan validation relatif kecil, yang menandakan model tidak mengalami overfitting secara signifikan. Hal ini menunjukkan bahwa model mampu belajar dengan baik dari data latih dan dapat melakukan generalisasi dengan baik terhadap data validasi. Secara keseluruhan, performa model dapat dikategorikan sangat baik dengan tingkat akurasi tinggi dan stabil setelah sekitar epoch ke-30.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi Model pada Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_acc = compute_mse_and_acc(______________) # Silakan diisi bagian ini dengan kode yang tepat (10)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kasus kesalahan prediksi (Plot failure cases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, probas = model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penjelasan Grafik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gambar di atas menampilkan contoh kasus kesalahan prediksi model pada dataset digit tulisan tangan. Terlihat bahwa sebagian besar error terjadi karena bentuk digit yang ambigu atau mirip dengan digit lain, misalnya angka **5** yang sering diprediksi sebagai **0, 3, 4, 7, atau 9**, serta angka **9** yang diprediksi sebagai **0, 3, atau 5**. Kesalahan juga muncul pada digit dengan goresan tidak jelas atau miring, sehingga menimbulkan interpretasi berbeda oleh model, seperti angka **2** yang diprediksi sebagai **4, 7, atau 8**. Pola kesalahan ini menunjukkan bahwa model masih kesulitan membedakan digit dengan bentuk serupa atau tulisan yang kurang rapi, yang merupakan karakteristik umum pada dataset MNIST. Dengan demikian, peningkatan akurasi model dapat diarahkan melalui teknik augmentasi data untuk variasi tulisan tangan, penggunaan arsitektur model yang lebih kompleks, atau penerapan metode regularisasi yang lebih baik agar model lebih robust terhadap variasi bentuk digit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model dengan Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_with_earlystopping(model, \n",
    "          X_train, y_train, \n",
    "          X_valid, y_valid, \n",
    "          num_epochs,\n",
    "          learning_rate=0.1,\n",
    "          # --- Early Stopping params ---\n",
    "          early_stopping=True,\n",
    "          monitor='valid_mse',     # 'valid_mse' (minimize) atau 'valid_acc' (maximize)\n",
    "          patience=10,             # berapa epoch tanpa perbaikan sebelum stop\n",
    "          min_delta=1e-4,          # ambang perbaikan minimal\n",
    "          restore_best_weights=True,\n",
    "          verbose=True):\n",
    "    \"\"\"\n",
    "    Train model dengan opsi Early Stopping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    epoch_loss: list[float]        # Train MSE per-epoch\n",
    "    epoch_train_acc: list[float]   # Train Acc (%) per-epoch\n",
    "    epoch_valid_acc: list[float]   # Valid Acc (%) per-epoch\n",
    "    best_epoch: int                # Epoch (0-based) dengan skor validasi terbaik\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper untuk snapshot & restore bobot model sederhana\n",
    "    def _snapshot_state(m):\n",
    "        return {\n",
    "            'weight_h': m.weight_h.copy(),\n",
    "            'bias_h':   m.bias_h.copy(),\n",
    "            'weight_out': m.weight_out.copy(),\n",
    "            'bias_out':   m.bias_out.copy()\n",
    "        }\n",
    "    def _restore_state(m, state):\n",
    "        m.weight_h[:] = state['weight_h']\n",
    "        m.bias_h[:]   = state['bias_h']\n",
    "        m.weight_out[:] = state['weight_out']\n",
    "        m.bias_out[:]   = state['bias_out']\n",
    "\n",
    "    # Konfigurasi mode (min untuk MSE, max untuk Acc)\n",
    "    if monitor == 'valid_acc':\n",
    "        mode = 'max'\n",
    "        best_score = -np.inf\n",
    "    else:\n",
    "        monitor = 'valid_mse'\n",
    "        mode = 'min'\n",
    "        best_score = np.inf\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    best_state = None\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        # melakukan iterasi pada minibatches (potongan kecil data latih)\n",
    "        minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "            #### Hitung keluaran (Compute outputs) ####\n",
    "            a_h, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Hitung gradien (Compute gradients) ####\n",
    "            d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = \\\n",
    "                model.backward(X_train_mini, a_h, a_out, y_train_mini)\n",
    "\n",
    "            #### Perbarui bobot (Update weights) ####\n",
    "            model.weight_h  -= learning_rate * d_loss__d_w_h\n",
    "            model.bias_h    -= learning_rate * d_loss__d_b_h\n",
    "            model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "            model.bias_out   -= learning_rate * d_loss__d_b_out\n",
    "\n",
    "        #### Pencatatan per-epoch (Epoch Logging) ####        \n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "        train_acc, valid_acc = train_acc*100, valid_acc*100\n",
    "\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "                  f'| Train MSE: {train_mse:.6f} '\n",
    "                  f'| Train Acc: {train_acc:.2f}% '\n",
    "                  f'| Valid MSE: {valid_mse:.6f} '\n",
    "                  f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "        # --- Early Stopping check ---\n",
    "        if early_stopping:\n",
    "            score = valid_acc if mode == 'max' else valid_mse\n",
    "\n",
    "            improved = (score - best_score) > min_delta if mode == 'max' else (best_score - score) > min_delta\n",
    "\n",
    "            if improved:\n",
    "                best_score = score\n",
    "                best_epoch = e\n",
    "                no_improve = 0\n",
    "                if restore_best_weights:\n",
    "                    best_state = _snapshot_state(model)\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    if verbose:\n",
    "                        print(f'Early stopping at epoch {e+1} (no improvement in {patience} epochs).')\n",
    "                    break\n",
    "\n",
    "    # Kembalikan bobot terbaik jika diminta\n",
    "    if restore_best_weights and best_state is not None:\n",
    "        _restore_state(model, best_state)\n",
    "        if verbose:\n",
    "            print(f'Restored best weights from epoch {best_epoch+1} with best {monitor}.')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versi lengkap dengan early stopping\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc, best_epoch = train_with_earlystopping(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.1,\n",
    "    early_stopping=True,\n",
    "    monitor='valid_mse',   # atau 'valid_acc'\n",
    "    patience=5,\n",
    "    min_delta=1e-4,        # jika monitor='valid_acc', contoh: 0.1 untuk 0.1 poin persen\n",
    "    restore_best_weights=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Best epoch:\", best_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n",
    "         label='Training')\n",
    "plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n",
    "         label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, probas = model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
